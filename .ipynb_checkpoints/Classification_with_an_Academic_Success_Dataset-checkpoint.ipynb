{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "0d84b60d-30a9-4cad-b3a4-72f45c52f28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#impprt packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import the Python machine learning libraries we need\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import graphviz \n",
    "from sklearn import tree\n",
    "import yaml\n",
    "import itertools\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import SGDClassifier, Perceptron, LogisticRegression, PassiveAggressiveClassifier, RidgeClassifierCV\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import RandomizedSearchCV,HalvingGridSearchCV\n",
    "# from sklearn.model_selection import HalvingGridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingClassifier, HistGradientBoostingClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.semi_supervised import LabelPropagation,LabelSpreading\n",
    "from sklearn.metrics import fbeta_score,confusion_matrix, f1_score, accuracy_score,precision_score, recall_score\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier, ExtraTreesClassifier, BaggingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.svm import NuSVC\n",
    "from sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from datetime import datetime, timedelta\n",
    "from xgboost import XGBClassifier\n",
    "import itertools\n",
    "import random\n",
    "import re\n",
    "import copy\n",
    "from imblearn.under_sampling import RandomUnderSampler \n",
    "from sklearn.ensemble import IsolationForest\n",
    "# from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "# from imblearn.combine import SMOTETomek, SMOTEENN\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier,BalancedBaggingClassifier\n",
    "\n",
    "from imbalanced_ensemble.ensemble import EasyEnsembleClassifier, SelfPacedEnsembleClassifier,OverBaggingClassifier,\\\n",
    "    RUSBoostClassifier, SMOTEBoostClassifier,KmeansSMOTEBoostClassifier, BalanceCascadeClassifier,UnderBaggingClassifier,\\\n",
    "    OverBoostClassifier,SMOTEBaggingClassifier,AdaUBoostClassifier,AdaCostClassifier,AsymBoostClassifier,\\\n",
    "    CompatibleAdaBoostClassifier,CompatibleBaggingClassifier\n",
    "# except:\n",
    "#     try:\n",
    "#         from imbens.ensemble import EasyEnsembleClassifier, SelfPacedEnsembleClassifier,OverBaggingClassifier,\\\n",
    "#         RUSBoostClassifier, SMOTEBoostClassifier,KmeansSMOTEBoostClassifier, BalanceCascadeClassifier,UnderBaggingClassifier,\\\n",
    "#         OverBoostClassifier,SMOTEBaggingClassifier,AdaUBoostClassifier,AdaCostClassifier,AsymBoostClassifier,\\\n",
    "#         CompatibleAdaBoostClassifier,CompatibleBaggingClassifier\n",
    "#     except:\n",
    "#         print(\"both failed\")\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import SGDClassifier, Perceptron, LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "import requests\n",
    "import json\n",
    "\n",
    "\n",
    "import yaml\n",
    "import time\n",
    "import numpy as np\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "13cabc87-3a72-467f-97d1-d777f6428241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "01ebde7e-aef7-4484-aeef-de8060567fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_datasets(train_data_path = \"train.csv\", test_data_path = \"test.csv\"):\n",
    "    train = pd.read_csv(train_data_path)\n",
    "    test = pd.read_csv(test_data_path)\n",
    "    return train, test\n",
    "\n",
    "def data_summary(dependent_variable = \"Target\"):\n",
    "    print(\"shape of training data: {}\".format(train.shape))\n",
    "    print(\"shape of test data: {}\".format(test.shape))\n",
    "    print(\"number of columns: {}\".format(len(train.columns)))\n",
    "    num_cols = list(train.select_dtypes(include='number').columns)\n",
    "    print(\"\\n{} numeric columns : {}\".format(len(num_cols), num_cols))\n",
    "    print(\"\\ndependent variable is : {}\".format(dependent_variable))\n",
    "    print(\"\\ndependent variable's category distribution : {}\".format(dependent_variable))\n",
    "    print(train[dependent_variable].value_counts())\n",
    "\n",
    "def boxPlotAll(df):\n",
    "    '''Show box plots for each feature'''\n",
    "    \n",
    "    # Select just the numeric features\n",
    "    df = df.select_dtypes(include=[np.number])\n",
    "\n",
    "    # Compute the layout grid size\n",
    "    data_cols = len(df.columns)\n",
    "    unit_size = 5\n",
    "    layout_cols = 4\n",
    "    layout_rows = int(data_cols/layout_cols+layout_cols)\n",
    "\n",
    "    # Make the plots\n",
    "    df.plot(kind='box', subplots=True, figsize=(layout_cols*unit_size,layout_rows*unit_size), layout=(layout_rows,layout_cols))\n",
    "\n",
    "    plt.show()  \n",
    "\n",
    "def histPlotAll(df):\n",
    "    '''Show histograms for each feature'''\n",
    "\n",
    "    # Select just the numeric features\n",
    "    df = df.select_dtypes(include=[np.number])\n",
    "\n",
    "    # Compute the layout grid size\n",
    "    data_cols = len(df.columns)\n",
    "    unit_size = 5\n",
    "    layout_cols = 4\n",
    "    layout_rows = int(data_cols/layout_cols+layout_cols)\n",
    "\n",
    "    # Make the plots\n",
    "    df.hist(figsize=(layout_cols*unit_size,layout_rows*unit_size), layout=(layout_rows,layout_cols))\n",
    "            \n",
    "    plt.show()   \n",
    "\n",
    "\n",
    "def correlationMatrix(df):\n",
    "    '''Show a correlation matrix for all features.'''\n",
    "    columns = df.select_dtypes(include=['float64','int64']).columns\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(df.corr(), vmin=-1, vmax=1, interpolation='none',cmap='RdYlBu')\n",
    "    fig.colorbar(cax)\n",
    "    ax.set_xticks(np.arange(len(columns)))\n",
    "    ax.set_yticks(np.arange(len(columns)))\n",
    "    ax.set_xticklabels(columns, rotation = 90)\n",
    "    ax.set_yticklabels(columns)\n",
    "    plt.show()   \n",
    "  \n",
    "\n",
    "def scatterMatrix(df):\n",
    "    '''Show a scatter matrix of all features.'''\n",
    "    unit_size = 5\n",
    "    pd.plotting.scatter_matrix(df,figsize=(unit_size*4, unit_size*4),  diagonal='kde')\n",
    "    plt.show()\n",
    "        \n",
    "\n",
    "def appendEqualCountsClass(df, class_name, feature, num_bins, labels):\n",
    "    '''Append a new class feature named 'class_name' based on a split of 'feature' into clases with equal sample points.  Class names are in 'labels'.'''\n",
    "\n",
    "    # Compute the bin boundaries\n",
    "    percentiles = np.linspace(0,100,num_bins+1)\n",
    "    bins = np.percentile(df[feature],percentiles)\n",
    "\n",
    "    # Split the data into bins\n",
    "    n = pd.cut(df[feature], bins = bins, labels=labels, include_lowest=True)\n",
    "\n",
    "    # Add the new binned feature to a copy of the data\n",
    "    c = df.copy()\n",
    "    c[class_name] = n\n",
    "    return c    \n",
    "\n",
    "def logisticRegressionSummary(model, column_names):\n",
    "    '''Show a summary of the trained logistic regression model'''\n",
    "\n",
    "    # Get a list of class names\n",
    "    numclasses = len(model.classes_)\n",
    "    if len(model.classes_)==2:\n",
    "        classes =  [model.classes_[1]] # if we have 2 classes, sklearn only shows one set of coefficients\n",
    "    else:\n",
    "        classes = model.classes_\n",
    "\n",
    "    # Create a plot for each class\n",
    "    for i,c in enumerate(classes):\n",
    "        # Plot the coefficients as bars\n",
    "        fig = plt.figure(figsize=(8,len(column_names)/3))\n",
    "        fig.suptitle('Logistic Regression Coefficients for Class ' + str(c), fontsize=16)\n",
    "        rects = plt.barh(column_names, model.coef_[i],color=\"lightblue\")\n",
    "        \n",
    "        # Annotate the bars with the coefficient values\n",
    "        for rect in rects:\n",
    "            width = round(rect.get_width(),4)\n",
    "            plt.gca().annotate('  {}  '.format(width),\n",
    "                        xy=(0, rect.get_y()),\n",
    "                        xytext=(0,2),  \n",
    "                        textcoords=\"offset points\",  \n",
    "                        ha='left' if width<0 else 'right', va='bottom')        \n",
    "        plt.show()\n",
    "        #for pair in zip(X.columns, model_lr.coef_[i]):\n",
    "        #    print (pair)\n",
    "\n",
    "def decisionTreeSummary(model, column_names):\n",
    "    '''Show a summary of the trained decision tree model'''\n",
    "\n",
    "    # Plot the feature importances as bars\n",
    "    fig = plt.figure(figsize=(8,len(column_names)/3))\n",
    "    fig.suptitle('Decision tree feature importance', fontsize=16)\n",
    "    rects = plt.barh(column_names, model.feature_importances_,color=\"khaki\")\n",
    "\n",
    "    # Annotate the bars with the feature importance values\n",
    "    for rect in rects:\n",
    "        width = round(rect.get_width(),4)\n",
    "        plt.gca().annotate('  {}  '.format(width),\n",
    "                    xy=(width, rect.get_y()),\n",
    "                    xytext=(0,2),  \n",
    "                    textcoords=\"offset points\",  \n",
    "                    ha='left', va='bottom')    \n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def linearRegressionSummary(model, column_names):\n",
    "    '''Show a summary of the trained linear regression model'''\n",
    "\n",
    "    # Plot the coeffients as bars\n",
    "    fig = plt.figure(figsize=(8,len(column_names)/3))\n",
    "    fig.suptitle('Linear Regression Coefficients', fontsize=16)\n",
    "    rects = plt.barh(column_names, model.coef_,color=\"lightblue\")\n",
    "\n",
    "    # Annotate the bars with the coefficient values\n",
    "    for rect in rects:\n",
    "        width = round(rect.get_width(),4)\n",
    "        plt.gca().annotate('  {}  '.format(width),\n",
    "                    xy=(0, rect.get_y()),\n",
    "                    xytext=(0,2),  \n",
    "                    textcoords=\"offset points\",  \n",
    "                    ha='left' if width<0 else 'right', va='bottom')        \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def viewDecisionTree(model, column_names):\n",
    "    '''Visualise the decision tree'''\n",
    "\n",
    "    dot_data = tree.export_graphviz(model, out_file=None,\n",
    "            feature_names=column_names,\n",
    "            class_names=model.classes_,\n",
    "            filled=True, rounded=True,\n",
    "            special_characters=True)\n",
    "    graph = graphviz.Source(dot_data) \n",
    "    return graph    \n",
    "\n",
    "\n",
    "def find_outliers(feature):\n",
    "    '''Return a list of outliers in the data'''\n",
    "\n",
    "    # Temporarily replace nulls with mean so they don't cause an error\n",
    "    feature = feature.fillna(feature.mean()) \n",
    "\n",
    "    # Compute the quartiles\n",
    "    quartile_1, quartile_3 = np.percentile(feature, [25, 75])\n",
    "\n",
    "    # Compute the inter-quartile range\n",
    "    iqr = quartile_3 - quartile_1\n",
    "\n",
    "    # Compute the outlier boundaries\n",
    "    lower_bound = quartile_1 - (iqr * 1.5)\n",
    "    upper_bound = quartile_3 + (iqr * 1.5)\n",
    "\n",
    "    # Return rows where the feature is outside the outlier boundaries\n",
    "    return np.where((feature > upper_bound) | (feature < lower_bound))\n",
    "\n",
    "def classComparePlot(df, class_name, plotType='density'):\n",
    "    '''Show comparative plots comparing the distribution of each feature for each class.  plotType can be 'density' or 'hist' '''\n",
    "\n",
    "    # Get the parameters for the plots\n",
    "    numcols = len(df.columns) - 1\n",
    "    unit_size = 5\n",
    "    classes = df[class_name].nunique()           # no of uniques classes\n",
    "    class_values = df[class_name].unique()       # unique class values\n",
    "\n",
    "    print('Comparative histograms for',class_values)\n",
    "    \n",
    "    # Make the plots\n",
    "    colors = plt.cm.get_cmap('tab10').colors\n",
    "    fig = plt.figure(figsize=(unit_size,numcols*unit_size))\n",
    "    ax = [None]*numcols \n",
    "    i = 0\n",
    "    for col_name in df.columns:\n",
    "        minVal = df[col_name].min()\n",
    "        maxVal = df[col_name].max()\n",
    "        \n",
    "        if col_name != class_name:                \n",
    "            ax[i] = fig.add_subplot(numcols,1,i+1)   \n",
    "            for j in range(classes):   \n",
    "                selectedCols = df[[col_name,class_name]]\n",
    "                filteredRows = selectedCols.loc[(df[class_name]==class_values[j])]\n",
    "                values = filteredRows[col_name]\n",
    "                values.plot(kind=plotType,ax=ax[i],color=[colors[j]], alpha = 0.8, label=class_values[j], range=(minVal,maxVal))\n",
    "                ax[i].set_title(col_name)\n",
    "                ax[i].grid() \n",
    "            ax[i].legend()\n",
    "            i += 1        \n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "5fd3bcb3-0983-4a9e-86d9-9628ca020e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = [\n",
    "    (\"Random Forest\", RandomForestClassifier(random_state=seed)),\n",
    "    (\"SVM Classifier\", SVC(random_state=seed)),\n",
    "    (\"SGD Classifier\", SGDClassifier(random_state=seed)),\n",
    "    (\"MLP Classifier\", MLPClassifier(random_state=seed)),\n",
    "    (\"Perceptron\", Perceptron(random_state=seed)),\n",
    "    (\"Logistic Regression\", LogisticRegression(random_state=seed)),\n",
    "    (\"Passive Aggressive Classifier\", PassiveAggressiveClassifier(random_state=seed)),\n",
    "    (\"Label Propagation\", LabelPropagation()),\n",
    "    (\"Label Spreading\", LabelSpreading()),\n",
    "    (\"Gradient Boosting\", GradientBoostingClassifier(random_state=seed)),\n",
    "    (\"Quadratic Discriminant Analysis\", QuadraticDiscriminantAnalysis()),\n",
    "    (\"HistGradientBoosting\", HistGradientBoostingClassifier(random_state=seed)),\n",
    "    (\"RidgeClassifierCV\", RidgeClassifierCV()),\n",
    "    (\"AdaBoostClassifier\", AdaBoostClassifier(random_state=seed)),\n",
    "    (\"ExtraTreesClassifier\", ExtraTreesClassifier(random_state=seed, class_weight='balanced')),\n",
    "    (\"KNeighborsClassifier\", KNeighborsClassifier()),\n",
    "    (\"BaggingClassifier\", BaggingClassifier(random_state=seed)),\n",
    "    (\"BernoulliNB\", BernoulliNB()),\n",
    "    (\"GaussianNB\", GaussianNB()),\n",
    "    (\"LinearDiscriminantAnalysis\", LinearDiscriminantAnalysis()),\n",
    "    (\"DecisionTreeClassifier\", DecisionTreeClassifier(random_state=seed)),\n",
    "    (\"NearestCentroid\", NearestCentroid()),\n",
    "    (\"ExtraTreeClassifier\", ExtraTreeClassifier(random_state=seed)),\n",
    "    (\"DummyClassifier\", DummyClassifier(strategy='stratified', random_state=seed)),\n",
    "    (\"Isolation Forest\", IsolationForest(random_state=seed)),\n",
    "    (\"Balanced Random Forest\", BalancedRandomForestClassifier(random_state=seed)),\n",
    "    (\"Easy Ensemble Classifier\", EasyEnsembleClassifier(random_state=seed)),\n",
    "    (\"RUSBoost Classifier\", RUSBoostClassifier(random_state=seed)),\n",
    "    (\"SMOTEBoost Classifier\", SMOTEBoostClassifier(random_state=seed)),\n",
    "    (\"SelfPaced Ensemble Classifier\", SelfPacedEnsembleClassifier(random_state=seed)),\n",
    "    (\"BalanceCascade Classifier\", BalanceCascadeClassifier(random_state=seed)),\n",
    "    (\"UnderBagging Classifier\", UnderBaggingClassifier(random_state=seed)),\n",
    "    (\"OverBoost Classifier\", OverBoostClassifier(random_state=seed)),\n",
    "    (\"KMeans SMOTEBoost Classifier\", KmeansSMOTEBoostClassifier(random_state=seed)),\n",
    "    (\"OverBagging Classifier\",OverBaggingClassifier(random_state=seed)),\n",
    "    (\"SMOTEBagging Classifier\",SMOTEBaggingClassifier(random_state=seed)),\n",
    "    (\"AdaUBoost Classifier\", AdaUBoostClassifier(random_state=seed)),\n",
    "    (\"AdaCost Classifier\",AdaCostClassifier(random_state=seed)),\n",
    "    (\"AsymBoost Classifier\", AsymBoostClassifier(random_state=seed)),\n",
    "    (\"Compatible AdaBoost Classifier\", CompatibleAdaBoostClassifier(random_state=seed)),\n",
    "    (\"Compatible Bagging Classifier\",CompatibleBaggingClassifier(random_state=seed)),\n",
    "    (\"XGBoost Classifier\", XGBClassifier(random_state = seed, objective = 'binary:logistic'))\n",
    "]\n",
    "\n",
    "\n",
    "classifiers_dict = {name: classifier for name, classifier in classifiers}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "4d2159ce-679f-4ad0-b05c-38af4221ab0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of training data: (76518, 38)\n",
      "shape of test data: (51012, 37)\n",
      "number of columns: 38\n",
      "\n",
      "37 numeric columns : ['id', 'Marital status', 'Application mode', 'Application order', 'Course', 'Daytime/evening attendance', 'Previous qualification', 'Previous qualification (grade)', 'Nacionality', \"Mother's qualification\", \"Father's qualification\", \"Mother's occupation\", \"Father's occupation\", 'Admission grade', 'Displaced', 'Educational special needs', 'Debtor', 'Tuition fees up to date', 'Gender', 'Scholarship holder', 'Age at enrollment', 'International', 'Curricular units 1st sem (credited)', 'Curricular units 1st sem (enrolled)', 'Curricular units 1st sem (evaluations)', 'Curricular units 1st sem (approved)', 'Curricular units 1st sem (grade)', 'Curricular units 1st sem (without evaluations)', 'Curricular units 2nd sem (credited)', 'Curricular units 2nd sem (enrolled)', 'Curricular units 2nd sem (evaluations)', 'Curricular units 2nd sem (approved)', 'Curricular units 2nd sem (grade)', 'Curricular units 2nd sem (without evaluations)', 'Unemployment rate', 'Inflation rate', 'GDP']\n",
      "\n",
      "dependent variable is : Target\n",
      "\n",
      "dependent variable's category distribution : Target\n",
      "Target\n",
      "Graduate    36282\n",
      "Dropout     25296\n",
      "Enrolled    14940\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "train,test = read_datasets()\n",
    "data_summary(dependent_variable = \"Target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "16d282d3-276c-49a7-bb9a-a9c22944274d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plots\n",
    "# histPlotAll(train)\n",
    "# boxPlotAll(train)\n",
    "# classComparePlot(train[['Target','Marital status', 'Application mode', 'Application order', 'Course', 'Daytime/evening attendance']], \"Target\", plotType='hist')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b38a34d1-7c30-43a4-9f64-177026b04a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_train_inputs(dropping_columns ,test_size = 0.25, dependent_variable = \"Target\", seed = 7):# variables to drop\n",
    "    to_drop = dropping_columns + [dependent_variable]\n",
    "    # train_processed = train.drop(columns=to_drop)\n",
    "    # test_processed=test .drop(columns=to_drop)\n",
    "    # Split into input and output features\n",
    "    y = train[dependent_variable]\n",
    "    X = train.loc[ : , ~train.columns.isin(to_drop)]\n",
    "    test_processed = test.loc[ : , ~test.columns.isin(to_drop)]\n",
    "    # Split into test and training sets\n",
    "    X_train, X_test, y_train, y_test =  train_test_split(X, y, test_size=test_size, random_state=seed)\n",
    "    return X_train, X_test, y_train, y_test, test_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "a88c09c2-4d6d-4e0e-876a-7ef25f610226",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_to_try = 5\n",
    "with open('config.yml', 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "base_model_grid_dict = [\n",
    "    {classifier: {param: value for param, value in zip(params_dict.keys(), param_combination)}}\n",
    "    for classifier, params_dict in config['model_grid'].items()\n",
    "    for param_combination in itertools.product(*params_dict.values())\n",
    "]\n",
    "base_model_grid_dict\n",
    "set1_tuples = [(key, value) for config in base_model_grid_dict for key, value in config.items()]\n",
    "new_models_to_try = random.sample(set1_tuples, models_to_try)\n",
    "# new_models_to_try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "21268fc8-d0e6-4e1a-91d0-ffcd918dca25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Balanced Random Forest',\n",
       "  {'n_estimators': 50,\n",
       "   'max_depth': 10,\n",
       "   'min_samples_split': 5,\n",
       "   'min_samples_leaf': 4,\n",
       "   'class_weight': 'balanced',\n",
       "   'max_features': 'log2'}),\n",
       " ('SMOTEBagging Classifier',\n",
       "  {'n_estimators': 200,\n",
       "   'max_samples': 1.0,\n",
       "   'k_neighbors': 1,\n",
       "   'max_features': 0.8,\n",
       "   'bootstrap_features': True,\n",
       "   'oob_score': False}),\n",
       " ('Random Forest',\n",
       "  {'n_estimators': 300,\n",
       "   'max_depth': 20,\n",
       "   'min_samples_split': 10,\n",
       "   'min_samples_leaf': 2,\n",
       "   'class_weight': 'balanced_subsample',\n",
       "   'max_features': 'sqrt'}),\n",
       " ('RUSBoost Classifier',\n",
       "  {'n_estimators': 200,\n",
       "   'learning_rate': 1.0,\n",
       "   'algorithm': 'SAMME.R',\n",
       "   'early_termination': True}),\n",
       " ('ExtraTreesClassifier',\n",
       "  {'n_estimators': 200,\n",
       "   'max_depth': None,\n",
       "   'min_samples_split': 10,\n",
       "   'min_samples_leaf': 4,\n",
       "   'class_weight': 'balanced',\n",
       "   'max_features': 'sqrt'})]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_models_to_try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "90462ed0-c9c2-4549-bf69-15307923c4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test, test_processed = test_train_inputs(dropping_columns = ['id'], test_size = 0.25, dependent_variable = \"Target\", seed = 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "55d7c246-1587-455f-a925-b0f2ad161186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "___________1th Classifier___________\n",
      "('Balanced Random Forest', {'n_estimators': 50, 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 4, 'class_weight': 'balanced', 'max_features': 'log2'})\n",
      "Balanced Random Forest\n",
      "0.7504356311423991\n",
      "0.7498693152117094\n",
      "\n",
      "___________2th Classifier___________\n",
      "('SMOTEBagging Classifier', {'n_estimators': 200, 'max_samples': 1.0, 'k_neighbors': 1, 'max_features': 0.8, 'bootstrap_features': True, 'oob_score': False})\n",
      "SMOTEBagging Classifier\n",
      "0.9998257475430403\n",
      "0.8268165185572399\n",
      "\n",
      "___________3th Classifier___________\n",
      "('Random Forest', {'n_estimators': 300, 'max_depth': 20, 'min_samples_split': 10, 'min_samples_leaf': 2, 'class_weight': 'balanced_subsample', 'max_features': 'sqrt'})\n",
      "Random Forest\n",
      "0.8959015822123092\n",
      "0.8233141662310507\n",
      "\n",
      "___________4th Classifier___________\n",
      "('RUSBoost Classifier', {'n_estimators': 200, 'learning_rate': 1.0, 'algorithm': 'SAMME.R', 'early_termination': True})\n",
      "RUSBoost Classifier\n",
      "0.8019272321739737\n",
      "0.803659174072138\n",
      "\n",
      "___________5th Classifier___________\n",
      "('ExtraTreesClassifier', {'n_estimators': 200, 'max_depth': None, 'min_samples_split': 10, 'min_samples_leaf': 4, 'class_weight': 'balanced', 'max_features': 'sqrt'})\n",
      "ExtraTreesClassifier\n",
      "0.8756534467135987\n",
      "0.8154208050182958\n"
     ]
    }
   ],
   "source": [
    "i = 0    \n",
    "for model in new_models_to_try:\n",
    "    i = i+1\n",
    "    print(\"\\n___________{}th Classifier___________\".format(i))\n",
    "    print(model)\n",
    "    classifier_name = model[0]\n",
    "    classifier = classifiers_dict[classifier_name]\n",
    "    param_dist = model[1]\n",
    "    print(classifier_name)\n",
    "    \n",
    "        \n",
    "    # Create and fit a model with the current hyperparameters\n",
    "    estimator = classifier.set_params(**param_dist)\n",
    "    estimator.fit(X_train, y_train)\n",
    "\n",
    "    # Check model performance on training data\n",
    "    predictions = estimator.predict(X_train)\n",
    "    print(accuracy_score(y_train, predictions))\n",
    "    \n",
    "    # Evaluate the model on the test data\n",
    "    predictions = estimator.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    print(accuracy)\n",
    "\n",
    "    # Evaluate the model on the test data\n",
    "    test_out = copy.deepcopy(test)\n",
    "    test_out['Target'] = estimator.predict(test_processed)\n",
    "    output = test_out[['id','Target']]\n",
    "    output.set_index(\"id\", inplace = True)\n",
    "\n",
    "    output.to_csv(\"submission_{}_{}.csv\".format(classifier_name,round(accuracy,2)))\n",
    "        \n",
    "    # # Select algorithm\n",
    "    # model = DecisionTreeClassifier()\n",
    "    \n",
    "    # # Fit model to the data\n",
    "    # model.fit(X_train, y_train)\n",
    "    \n",
    "    # # Check model performance on training data\n",
    "    # predictions = model.predict(X_train)\n",
    "    # print(accuracy_score(y_train, predictions))\n",
    "    \n",
    "    # # Evaluate the model on the test data\n",
    "    # predictions = model.predict(X_test)\n",
    "    # print(accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b6a732-6e6a-49ca-bf91-904a92d2d607",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
